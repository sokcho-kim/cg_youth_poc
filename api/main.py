from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import os
import sys
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
dotenv_path = os.path.join(BASE_DIR, ".env")
load_dotenv(dotenv_path)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
import json

app = FastAPI(
    title="ì²­ë…„ì •ì±… ê²€ìƒ‰ API",
    description="ì„œìš¸ì‹œ ì²­ë…„ì •ì±… ê²€ìƒ‰ ë° AI ë‹µë³€ ìƒì„± API",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ê°œë°œìš©. í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
vectorstore = None
embedding_model = None
openai_client = None

class SearchRequest(BaseModel):
    query: str
    k: int = 5
    filters: Optional[Dict[str, Any]] = None

class SearchResponse(BaseModel):
    query: str
    results: List[Dict[str, Any]]
    total_count: int

class AnswerRequest(BaseModel):
    query: str
    user_context: Optional[str] = None
    k: int = 3

class AnswerResponse(BaseModel):
    query: str
    answer: str
    sources: List[Dict[str, Any]]
    confidence: float

def load_vectorstore():
    """ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ"""
    global vectorstore, embedding_model, openai_client
    
    try:
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        embedding_model = HuggingFaceEmbeddings(model_name="BM-K/KoSimCSE-roberta")
        
        # ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
        vectorstore = Chroma(
            persist_directory="vectorstore",
            embedding_function=embedding_model
        )
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        openai_api_key = os.getenv('OPENAI_API_KEY')
        if openai_api_key:
            try:
                from openai import OpenAI
                openai_client = OpenAI(api_key=openai_api_key)
                print("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except ImportError:
                print("âŒ openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. OpenAI ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
        
        print("âœ… ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
        print(f"âœ… .env ê²½ë¡œ: {dotenv_path}")
        print(f"âœ… OPENAI_API_KEY: {os.getenv('OPENAI_API_KEY')}")
        return True
    except Exception as e:
        print(f"âŒ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

def generate_llm_answer(query: str, context_docs: List[Document]) -> str:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
    OpenAI API ë˜ëŠ” í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€ ìƒì„±
    """
    if not context_docs:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ì±…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    if openai_client:
        try:
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context_parts = []
            for i, doc in enumerate(context_docs, 1):
                context_parts.append(f"""
ì •ì±… {i}: {doc.metadata.get('title', 'ì œëª© ì—†ìŒ')}
ì£¼ê´€ê¸°ê´€: {doc.metadata.get('agency', '')}
ì—°ë ¹: {doc.metadata.get('age_range', '')}
ì‹ ì²­ê¸°ê°„: {doc.metadata.get('apply_start', '')} ~ {doc.metadata.get('apply_end', '')}
ì§€ì›ë‚´ìš©: {doc.page_content[:300]}...
ì‹ ì²­ì‚¬ì´íŠ¸: {doc.metadata.get('application_site', '')}
""")
            
            context = "\n".join(context_parts)
            
            # OpenAI API í˜¸ì¶œ (GPT-4o-mini ì‚¬ìš©)
            model_name = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')
            response = openai_client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì„œìš¸ì‹œ ì²­ë…„ ì •ì±… ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": f"ì§ˆë¬¸: {query}\n\nê´€ë ¨ ì •ì±… ì •ë³´:\n{context}\n\nìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€ìœ¼ë¡œ fallback
    
    # í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€ (fallback ë˜ëŠ” OpenAI ì—†ì„ ë•Œ)
    policies = []
    for doc in context_docs:
        policy_info = {
            "title": doc.metadata.get("title", "ì œëª© ì—†ìŒ"),
            "agency": doc.metadata.get("agency", ""),
            "age_range": doc.metadata.get("age_range", ""),
            "policy_type": doc.metadata.get("policy_type", ""),
            "apply_period": f"{doc.metadata.get('apply_start', '')} ~ {doc.metadata.get('apply_end', '')}",
            "application_site": doc.metadata.get("application_site", ""),
            "summary": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
        }
        policies.append(policy_info)
    
    answer_parts = []
    answer_parts.append(f"'{query}'ì— ëŒ€í•œ ê´€ë ¨ ì •ì±…ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:")
    answer_parts.append("")
    
    for i, policy in enumerate(policies, 1):
        answer_parts.append(f"ğŸ“‹ {i}. {policy['title']}")
        if policy['agency']:
            answer_parts.append(f"   ì£¼ê´€ê¸°ê´€: {policy['agency']}")
        if policy['age_range']:
            answer_parts.append(f"   ì—°ë ¹: {policy['age_range']}")
        if policy['apply_period'] and policy['apply_period'] != " ~ ":
            answer_parts.append(f"   ì‹ ì²­ê¸°ê°„: {policy['apply_period']}")
        if policy['application_site']:
            answer_parts.append(f"   ì‹ ì²­ì‚¬ì´íŠ¸: {policy['application_site']}")
        answer_parts.append(f"   ìš”ì•½: {policy['summary']}")
        answer_parts.append("")
    
    answer_parts.append("ğŸ’¡ ë” ìì„¸í•œ ì •ë³´ëŠ” ê° ì •ì±…ì˜ ì‹ ì²­ì‚¬ì´íŠ¸ë¥¼ ë°©ë¬¸í•´ë³´ì„¸ìš”!")
    
    return "\n".join(answer_parts)

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ"""
    if not load_vectorstore():
        raise Exception("ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨")

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "ì²­ë…„ì •ì±… ê²€ìƒ‰ API",
        "version": "1.0.0",
        "endpoints": {
            "search": "/search",
            "answer": "/answer",
            "health": "/health"
        }
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "vectorstore_loaded": vectorstore is not None
    }

@app.post("/search", response_model=SearchResponse)
async def search_policies(request: SearchRequest):
    """
    ì •ì±… ê²€ìƒ‰ API
    
    Args:
        request: ê²€ìƒ‰ ìš”ì²­ (ì¿¼ë¦¬, ê²°ê³¼ ìˆ˜, í•„í„°)
    
    Returns:
        ê²€ìƒ‰ ê²°ê³¼
    """
    if not vectorstore:
        raise HTTPException(status_code=500, detail="ë²¡í„°ìŠ¤í† ì–´ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        # ê²€ìƒ‰ ì‹¤í–‰
        results = vectorstore.similarity_search(
            request.query, 
            k=request.k
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        for doc in results:
            result = {
                "title": doc.metadata.get("title", ""),
                "policy_id": doc.metadata.get("policy_id", ""),
                "policy_type": doc.metadata.get("policy_type", ""),
                "agency": doc.metadata.get("agency", ""),
                "age_range": doc.metadata.get("age_range", ""),
                "education": doc.metadata.get("education", ""),
                "employment_status": doc.metadata.get("employment_status", ""),
                "apply_start": doc.metadata.get("apply_start", ""),
                "apply_end": doc.metadata.get("apply_end", ""),
                "support_scale": doc.metadata.get("support_scale", ""),
                "page_url": doc.metadata.get("page_url", ""),
                "application_site": doc.metadata.get("application_site", ""),
                "content": doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content
            }
            formatted_results.append(result)
        
        return SearchResponse(
            query=request.query,
            results=formatted_results,
            total_count=len(formatted_results)
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/answer", response_model=AnswerResponse)
async def generate_answer(request: AnswerRequest):
    """
    LLM ë‹µë³€ ìƒì„± API
    
    Args:
        request: ë‹µë³€ ìƒì„± ìš”ì²­ (ì¿¼ë¦¬, ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸, ê²€ìƒ‰ ê²°ê³¼ ìˆ˜)
    
    Returns:
        AI ë‹µë³€ê³¼ ì†ŒìŠ¤ ì •ë³´
    """
    if not vectorstore:
        raise HTTPException(status_code=500, detail="ë²¡í„°ìŠ¤í† ì–´ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        # ê´€ë ¨ ì •ì±… ê²€ìƒ‰
        context_docs = vectorstore.similarity_search(
            request.query, 
            k=request.k
        )
        
        # LLM ë‹µë³€ ìƒì„±
        answer = generate_llm_answer(request.query, context_docs)
        
        # ì†ŒìŠ¤ ì •ë³´ ì •ë¦¬
        sources = []
        for doc in context_docs:
            source = {
                "title": doc.metadata.get("title", ""),
                "policy_id": doc.metadata.get("policy_id", ""),
                "agency": doc.metadata.get("agency", ""),
                "page_url": doc.metadata.get("page_url", ""),
                "application_site": doc.metadata.get("application_site", ""),
                "relevance_score": 0.95  # ì‹¤ì œë¡œëŠ” ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° í•„ìš”
            }
            sources.append(source)
        
        return AnswerResponse(
            query=request.query,
            answer=answer,
            sources=sources,
            confidence=0.9 if sources else 0.0
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/policy/{policy_id}")
async def get_policy_detail(policy_id: str):
    """
    íŠ¹ì • ì •ì±… ìƒì„¸ ì •ë³´ ì¡°íšŒ
    
    Args:
        policy_id: ì •ì±… ID
    
    Returns:
        ì •ì±… ìƒì„¸ ì •ë³´
    """
    try:
        # JSON íŒŒì¼ì—ì„œ ì •ì±… ì •ë³´ ë¡œë“œ
        policy_file = f"data/processed/{policy_id}.json"
        if not os.path.exists(policy_file):
            raise HTTPException(status_code=404, detail="ì •ì±…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        with open(policy_file, 'r', encoding='utf-8') as f:
            policy_data = json.load(f)
        
        return policy_data
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì •ì±… ì •ë³´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 