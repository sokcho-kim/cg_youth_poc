from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import requests
from bs4 import BeautifulSoup
import json
import time
import os
import re

BASE_LIST_URL = "https://youth.seoul.go.kr/infoData/plcyInfo/list.do"
BASE_VIEW_URL = "https://youth.seoul.go.kr/infoData/plcyInfo/view.do"

PARAMS = {
    "sc_plcyFldCd": "023010",  # ì¼ìë¦¬ ë¶„ì•¼
    "orderBy": "regYmd desc"
}

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36"
}

SAVE_PATH = "data/processed"
os.makedirs(SAVE_PATH, exist_ok=True)

def get_policy_ids_selenium(page=1):
    options = webdriver.ChromeOptions()
    # options.add_argument("--headless=new")  # ë””ë²„ê¹…ì„ ìœ„í•´ ì£¼ì„ ì²˜ë¦¬
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    # Windows í™˜ê²½ì—ì„œ ChromeDriver ê²½ë¡œ ì„¤ì •
    chrome_driver_path = r"C:\Users\sokch\Downloads\chromedriver-win64\chromedriver-win64\chromedriver.exe"
    service = Service(executable_path=chrome_driver_path)
    
    driver = webdriver.Chrome(service=service, options=options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    url = f"{BASE_LIST_URL}?sc_plcyFldCd=023010&pageIndex={page}&orderBy=regYmd+desc"
    print(f"ğŸ”— ì ‘ì† URL: {url}")
    driver.get(url)
    
    # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
    wait = WebDriverWait(driver, 10)
    try:
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".policy-list")))
        print("âœ… í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
    except:
        print("âŒ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨")
        # HTML ì €ì¥í•´ì„œ ë””ë²„ê¹…
        with open(f"debug_page_{page}.html", "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        print(f"ğŸ“„ HTML ì €ì¥ë¨: debug_page_{page}.html")
    
    time.sleep(3)

    # ì •ì±… ëª©ë¡ì—ì„œ ID ì¶”ì¶œ
    print("ğŸ” ì •ì±… ëª©ë¡ì—ì„œ ID ì¶”ì¶œ ì¤‘...")
    
    # ì„œìš¸ì‹œ ì •ì±… ëª©ë¡
    seoul_policies = driver.find_elements(By.CSS_SELECTOR, ".policy-list li a[onclick*='goView']")
    print(f"   - ì„œìš¸ì‹œ ì •ì±… ìˆ˜: {len(seoul_policies)}")
    
    ids = []
    for policy in seoul_policies:
        try:
            onclick = policy.get_attribute("onclick")
            if onclick:
                match = re.search(r"goView\('([^']+)'\)", onclick)
                if match:
                    policy_id = match.group(1)
                    ids.append(policy_id)
                    print(f"   âœ… ì„œìš¸ì‹œ ì •ì±… ID ì°¾ìŒ: {policy_id}")
        except Exception as e:
            print(f"   âŒ ì •ì±… ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    # ì¤‘ë³µ ì œê±°
    ids = list(set(ids))
    print(f"ğŸ”¹ ì´ ê³ ìœ  ID ìˆ˜: {len(ids)}")
    
    driver.quit()
    return ids

def parse_detail(policy_id):
    res = requests.get(BASE_VIEW_URL, params={"plcyBizId": policy_id}, headers=HEADERS)
    soup = BeautifulSoup(res.text, "html.parser")

    title = soup.select_one(".policy-title h3")
    title = title.text.strip() if title else ""

    info_table = soup.select(".table-wrap table tr")
    data = {
        "title": title,
        "plcyBizId": policy_id,
        "tags": ["ì¼ìë¦¬"],
        "page_url": f"{BASE_VIEW_URL}?plcyBizId={policy_id}"
    }
    
    for row in info_table:
        th = row.select_one("th")
        td = row.select_one("td")
        if not th or not td:
            continue
        label = th.text.strip()
        value = td.text.strip().replace("\xa0", " ")

        if "ì‹ ì²­ê¸°ê°„" in label:
            if "~" in value:
                data["apply_start"], data["apply_end"] = [v.strip() for v in value.split("~")]
            else:
                data["apply_start"] = value
                data["apply_end"] = ""
        elif "ì§€ì›ëŒ€ìƒ" in label:
            data["target"] = value
        elif "ì£¼ê´€ê¸°ê´€" in label or "ë‹´ë‹¹ê¸°ê´€" in label:
            data["agency"] = value
        elif "ì²¨ë¶€" in label:
            file_link = td.select_one("a")
            if file_link and file_link.has_attr("href"):
                data["file_url"] = "https://youth.seoul.go.kr" + str(file_link["href"])
        elif "ì§€ì›ë‚´ìš©" in label:
            data["content"] = value

    return data

def save_json(data):
    fname = os.path.join(SAVE_PATH, f"{data['plcyBizId']}.json")
    with open(fname, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    all_ids = []
    max_pages = 1  # ë””ë²„ê¹…ì„ ìœ„í•´ 1í˜ì´ì§€ë§Œ ì‹œë„
    
    for page in range(1, max_pages + 1):
        print(f"ğŸ“„ {page}í˜ì´ì§€ ì •ì±…ID ìˆ˜ì§‘ ì¤‘...")
        ids = get_policy_ids_selenium(page)
        print(f"ğŸ”¹ ìˆ˜ì§‘ëœ ID ìˆ˜: {len(ids)}")
        if not ids:
            print("âŒ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. HTML íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            break
        all_ids.extend(ids)
        time.sleep(1)

    if all_ids:
        print(f"ğŸ” ì´ {len(all_ids)}ê±´ ìˆ˜ì§‘ ì‹œì‘...")
        for pid in all_ids:
            print(f"â–¶ {pid} ìˆ˜ì§‘ ì¤‘...")
            try:
                detail = parse_detail(pid)
                save_json(detail)
                print(f"âœ… ì €ì¥ ì™„ë£Œ: {detail['title']}")
            except Exception as e:
                print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            time.sleep(1)
    else:
        print("âŒ ìˆ˜ì§‘í•  IDê°€ ì—†ìŠµë‹ˆë‹¤.")
