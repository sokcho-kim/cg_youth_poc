from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import requests
from bs4 import BeautifulSoup
import json
import time
import os
import re
from tqdm import tqdm

BASE_LIST_URL = "https://youth.seoul.go.kr/infoData/plcyInfo/list.do"
BASE_VIEW_URL = "https://youth.seoul.go.kr/infoData/plcyInfo/view.do"

PARAMS = {
    "sc_plcyFldCd": "023010",  # ì¼ìë¦¬ ë¶„ì•¼
    "orderBy": "regYmd desc"
}

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36"
}

SAVE_PATH = "data/processed"
os.makedirs(SAVE_PATH, exist_ok=True)

CATEGORIES = {
    "ì „ì²´": "",
    "ì¼ìë¦¬": "023010",
    "ì£¼ê±°": "023020",
    "êµìœ¡": "023030",
    "ë³µì§€": "023040",
    "ì°¸ì—¬": "023050",
    "ë¬¸í™”": "023060",
    # í•„ìš”ì‹œ ë” ì¶”ê°€
}
MAX_PAGES_PER_CATEGORY = 100

def get_policy_ids_selenium(category_code, page=1):
    options = webdriver.ChromeOptions()
    # options.add_argument("--headless=new")  # ë””ë²„ê¹…ì„ ìœ„í•´ ì£¼ì„ ì²˜ë¦¬
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    # Windows í™˜ê²½ì—ì„œ ChromeDriver ê²½ë¡œ ì„¤ì •
    chrome_driver_path = r"C:\Users\sokch\Downloads\chromedriver-win64\chromedriver-win64\chromedriver.exe"
    service = Service(executable_path=chrome_driver_path)
    
    driver = webdriver.Chrome(service=service, options=options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    url = f"{BASE_LIST_URL}?sc_plcyFldCd={category_code}&pageIndex={page}&orderBy=regYmd+desc"
    print(f"ğŸ”— ì ‘ì† URL: {url}")
    driver.get(url)
    
    # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
    wait = WebDriverWait(driver, 10)
    try:
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".policy-list")))
        print("âœ… í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
    except:
        print("âŒ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨")
        # HTML ì €ì¥í•´ì„œ ë””ë²„ê¹…
        with open(f"debug_page_{page}.html", "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        print(f"ğŸ“„ HTML ì €ì¥ë¨: debug_page_{page}.html")
    
    time.sleep(3)

    # ì •ì±… ëª©ë¡ì—ì„œ ID ì¶”ì¶œ
    print("ğŸ” ì •ì±… ëª©ë¡ì—ì„œ ID ì¶”ì¶œ ì¤‘...")
    
    # ì„œìš¸ì‹œ ì •ì±… ëª©ë¡
    seoul_policies = driver.find_elements(By.CSS_SELECTOR, ".policy-list li a[onclick*='goView']")
    print(f"   - ì„œìš¸ì‹œ ì •ì±… ìˆ˜: {len(seoul_policies)}")
    
    ids = []
    for policy in seoul_policies:
        try:
            onclick = policy.get_attribute("onclick")
            if onclick:
                match = re.search(r"goView\('([^']+)'\)", onclick)
                if match:
                    policy_id = match.group(1)
                    ids.append(policy_id)
                    print(f"   âœ… ì„œìš¸ì‹œ ì •ì±… ID ì°¾ìŒ: {policy_id}")
        except Exception as e:
            print(f"   âŒ ì •ì±… ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    # ì¤‘ë³µ ì œê±°
    ids = list(set(ids))
    print(f"ğŸ”¹ ì´ ê³ ìœ  ID ìˆ˜: {len(ids)}")
    
    driver.quit()
    return ids

def parse_detail(policy_id):
    res = requests.get(BASE_VIEW_URL, params={"plcyBizId": policy_id}, headers=HEADERS)
    soup = BeautifulSoup(res.text, "html.parser")

    title = soup.select_one(".policy-title h3")
    title = title.text.strip() if title else ""

    info_table = soup.select(".table-wrap table tr")
    data = {
        "title": title,
        "plcyBizId": policy_id,
        "tags": ["ì¼ìë¦¬"],
        "page_url": f"{BASE_VIEW_URL}?plcyBizId={policy_id}"
    }
    
    for row in info_table:
        th = row.select_one("th")
        td = row.select_one("td")
        if not th or not td:
            continue
        label = th.text.strip()
        value = td.text.strip().replace("\xa0", " ")

        if "ì‹ ì²­ê¸°ê°„" in label:
            if "~" in value:
                data["apply_start"], data["apply_end"] = [v.strip() for v in value.split("~")]
            else:
                data["apply_start"] = value
                data["apply_end"] = ""
        elif "ì§€ì›ëŒ€ìƒ" in label:
            data["target"] = value
        elif "ì£¼ê´€ê¸°ê´€" in label or "ë‹´ë‹¹ê¸°ê´€" in label:
            data["agency"] = value
        elif "ì²¨ë¶€" in label:
            file_link = td.select_one("a")
            if file_link and file_link.has_attr("href"):
                data["file_url"] = "https://youth.seoul.go.kr" + str(file_link["href"])
        elif "ì§€ì›ë‚´ìš©" in label:
            data["content"] = value

    return data

def save_json(data):
    fname = os.path.join(SAVE_PATH, f"{data['plcyBizId']}.json")
    if os.path.exists(fname):
        return  # ì´ë¯¸ ìˆìœ¼ë©´ ê±´ë„ˆëœ€
    try:
        with open(fname, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {fname}")
    except Exception as e:
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {fname}, ì—ëŸ¬: {e}")

def save_id_list(id_list, cat_name):
    with open(f"policy_ids_{cat_name}.json", "w", encoding="utf-8") as f:
        json.dump(list(id_list), f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    for cat_name, cat_code in CATEGORIES.items():
        print(f"\n=== [{cat_name}] ë¶„ì•¼ í¬ë¡¤ë§ ì‹œì‘ ===")
        all_ids = set()
        for page in tqdm(range(1, MAX_PAGES_PER_CATEGORY + 1), desc=f"{cat_name} í˜ì´ì§€"):
            ids = get_policy_ids_selenium(cat_code, page)
            if not ids:
                print(f"ğŸš© {cat_name} {page}í˜ì´ì§€ì— ì •ì±…ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ ì´ë™.")
                break
            for pid in ids:
                if pid not in all_ids:
                    all_ids.add(pid)
                    try:
                        detail = parse_detail(pid)
                        save_json(detail)
                    except Exception as e:
                        print(f"âŒ {pid} ìƒì„¸ ìˆ˜ì§‘ ì—ëŸ¬: {e}")
            # 100ê±´ë§ˆë‹¤ ì •ì±… ID ëª©ë¡ ì €ì¥
            if len(all_ids) % 100 == 0:
                save_id_list(all_ids, cat_name)
            time.sleep(1)
        # ì¹´í…Œê³ ë¦¬ ëë‚  ë•Œë§ˆë‹¤ ID ëª©ë¡ ì €ì¥
        save_id_list(all_ids, cat_name)
        print(f"âœ… {cat_name} ë¶„ì•¼ ì •ì±… {len(all_ids)}ê±´ ì €ì¥ ì™„ë£Œ")
